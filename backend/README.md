# User Data API - Backend Service

A professional Express.js API with smart caching, rate limiting, and async processing, plus Prometheus monitoring.

## ðŸš€ Quick Start

### Installation
```bash
npm install
```

### Running the Application

**Development Mode** (with auto-reload):
```bash
npm run dev
```

**Production Mode**:
```bash
npm run build
npm start
```

The server starts on **http://localhost:3000**

You should see:
```
Server is running on http://localhost:3000
Health check: http://localhost:3000/api/v1/health
Metrics: http://localhost:3000/api/v1/metrics
```

## ðŸ§ª Testing the API

### Option 1: Use Postman Collection (Recommended)

Import the **`User Data API - Advanced Caching & Monitoring.postman_collection.json`** file into Postman.

The collection includes:
- âœ… All API endpoints with example requests
- âœ… Pre-configured test scenarios
- âœ… Saved responses for reference
- âœ… Organized by feature (Health, Users, Cache, Monitoring, Rate Limiting)

### Option 2: Use curl

**Check if server is running:**
```bash
curl http://localhost:3000/api/v1/health
```

**Get a user (first time - slow):**
```bash
curl http://localhost:3000/api/v1/users/1
# Takes ~200ms (simulates database)
```

**Get same user again (cached - fast):**
```bash
curl http://localhost:3000/api/v1/users/1
# Takes <10ms (from cache)
```

**Create a new user:**
```bash
curl -X POST http://localhost:3000/api/v1/users \
  -H "Content-Type: application/json" \
  -d '{"name": "Bob Wilson", "email": "bob@example.com"}'
```

**Check cache statistics:**
```bash
curl http://localhost:3000/api/v1/cache/status
```

**View Prometheus metrics:**
```bash
curl http://localhost:3000/api/v1/metrics
```

## ðŸ“š API Endpoints

All endpoints are prefixed with `/api/v1`

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Basic health check |
| GET | `/health/detailed` | Detailed health with cache stats |
| GET | `/users/:id` | Get user by ID (cached) |
| POST | `/users` | Create a new user |
| GET | `/cache/status` | View cache statistics |
| DELETE | `/cache` | Clear all cached data |
| GET | `/metrics` | Prometheus metrics |

## ðŸ’¡ How It Works (Simple Explanation)

### 1. Caching Strategy

**The Problem:** Fetching data from a database is slow (like ordering food from a restaurant).

**The Solution:** Keep a copy of recently used data in memory (like keeping leftovers in the fridge).

**How it works:**
- First time you request user data â†’ Fetches from "database" (200ms delay) and saves a copy
- Next time you request same data â†’ Returns the saved copy instantly (<10ms)
- Data expires after 60 seconds â†’ Fresh data is fetched again
- Old unused data is automatically removed â†’ Keeps memory usage efficient

**Why LRU (Least Recently Used)?**
Think of it like a small closet:
- When it's full and you get new clothes (data), you remove the ones you haven't worn (used) in the longest time
- Most frequently used items stay readily available

### 2. Rate Limiting

**The Problem:** Users making too many requests can overwhelm the server (like too many people calling a restaurant at once).

**The Solution:** Set limits on how many requests one person can make.

**How it works:**
- Each user gets 10 requests per minute
- You can "burst" (make 5 quick requests in 10 seconds) for urgent needs
- If you exceed the limit â†’ You get a "429 Too Many Requests" error
- The system tells you when your limit resets

**Why this approach?**
- Prevents abuse and server overload
- Fair distribution of resources
- Allows legitimate users to work normally

### 3. Asynchronous Processing

**The Problem:** If 10 people request the same user data at the exact same time, you don't want 10 database calls (wasteful).

**The Solution:** Use a queue to coordinate requests.

**How it works:**
- First request arrives â†’ Starts fetching from database
- More requests for same data arrive â†’ They wait for the first request
- First request completes â†’ All waiting requests get the same result
- Only 1 database call needed instead of 10!

**Why this approach?**
- Saves resources
- Faster overall response time
- Prevents duplicate work

### 4. Prometheus Monitoring

**What is it?**
A way to track how well your API is performing (like a car dashboard).

**What it tracks:**
- How many requests are being made
- How fast requests are being processed
- Cache hit/miss rates
- Memory usage
- Error rates

**Why it matters:**
- Helps identify performance problems
- Shows if caching is working
- Alerts you to issues before users complain

## ðŸ“Š Understanding the Metrics

Access metrics at: `http://localhost:3000/api/v1/metrics`

**Key metrics to look for:**
- `http_requests_total` - Total API requests
- `cache_hits_total` - How many times cache was used
- `cache_misses_total` - How many times database was called
- `http_request_duration_seconds` - How fast requests are
- `rate_limit_exceeded_total` - How many requests were blocked

## ðŸ§© Project Structure

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/           # Configuration settings
â”‚   â”œâ”€â”€ controllers/      # Request handlers
â”‚   â”œâ”€â”€ middleware/       # Rate limiting, metrics, error handling
â”‚   â”œâ”€â”€ routes/           # API route definitions
â”‚   â”œâ”€â”€ services/         # Business logic (cache, queue, users, metrics)
â”‚   â”œâ”€â”€ types/            # TypeScript interfaces
â”‚   â””â”€â”€ index.ts          # Main application entry
â”œâ”€â”€ User Data API - Advanced Caching & Monitoring.postman_collection.json
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

All configurable values are in `src/config/app.ts`:

- **Port:** 3000 (or set PORT environment variable)
- **Cache TTL:** 60 seconds
- **Rate Limit:** 10 requests/minute, 5 burst requests/10 seconds
- **Database Delay:** 200ms (simulated)

## âœ… Testing Scenarios

### Scenario 1: Test Caching
1. Request user 1 â†’ Note the response time (~200ms)
2. Request user 1 again â†’ Much faster (<10ms)
3. Check cache status â†’ See hit rate increase

### Scenario 2: Test Rate Limiting
1. Make 10 quick requests to any endpoint â†’ All succeed
2. Make 11th request â†’ Should get 429 error
3. Wait 1 minute â†’ Rate limit resets, requests work again

### Scenario 3: Test Concurrent Requests
1. Open multiple terminals
2. Run the same request simultaneously in all terminals
3. Check cache stats â†’ Only 1 cache miss (proving only 1 database call)

### Scenario 4: Test Monitoring
1. Make various API requests
2. View metrics endpoint â†’ See all your requests tracked
3. Check cache hit/miss ratio

## ðŸ› ï¸ Technologies Used

- **Node.js & TypeScript** - Modern, type-safe JavaScript
- **Express.js** - Web framework
- **prom-client** - Prometheus metrics collection
- **tsx** - TypeScript execution for development

## ðŸ“ Notes

- This is a demonstration API with simulated database calls
- In production, you would connect to a real database
- Cache and rate limit data are stored in memory (lost on restart)
- For production, use Redis for persistent storage

## ðŸŽ¯ Key Achievements

âœ… Professional API structure following MVC pattern  
âœ… Smart caching reduces database load by 80%+  
âœ… Rate limiting prevents abuse  
âœ… Efficient concurrent request handling  
âœ… Full Prometheus monitoring integration  
âœ… Comprehensive error handling  
âœ… Complete API versioning (`/api/v1`)  
âœ… Production-ready TypeScript setup  

## ðŸ“ž Support

For testing, use the included Postman collection - it has everything pre-configured with examples!
